{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS EDA Project ⟡ King County House Prices ⟡ *Amy Williams* Edition\n",
    "\n",
    "## Outline of the Problem\n",
    "\n",
    "Amy Williams (Seller, Mafiosa) sells several central houses (top10%) over time, needs average outskirt houses over time to hide from the FBI.\n",
    "\n",
    "Corrected version (since Miss Williams is  quite upset that her smart tricks are misconstrued as organised crime ...):\n",
    "\n",
    "Amy Williams, a businesswoman by trade\n",
    "- wants to sells several central houses (top 10%) over time and\n",
    "- needs average outskirt houses over time to avoid unwanted attention from the FBI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usual Setup Tasks\n",
    "\n",
    "To analyse the data set, we begin by loading the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Required for some plot fine tuning.\n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the data set under consideration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the csv file containing the data.\n",
    "DATAPATH = \"./data/King_County_House_prices_dataset.csv\"\n",
    "\n",
    "df_0 = pd.read_csv(DATAPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Inspection and Data Cleaning\n",
    "\n",
    "For an initial overview of this data set, let us perform the usual steps of inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 10 rows of the table.\n",
    "df_0.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about column dtypes and NaN values. \n",
    "df_0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common statistical quantities for the numerical columns.\n",
    "df_0.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the necessary cleaning tasks. For convenience, let us create a deep copy of the original data that will incorporate our edits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fix the data types for several columns where necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"date\"\n",
    "\n",
    "As we saw, the displayed `dtype` of \"date\" is `object`. \n",
    "\n",
    "Dealing with dates becomes much easier if we cast colum\n",
    "\n",
    "It is always advisable to use the available tools for working with dates. In pandas, this means to casting the affected columns into the datetime type. \n",
    "\n",
    "Fortunately, there are no further intricacies (like exotic date formats) so that the following command suffices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"date\"] = pd.to_datetime(df_0[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"bedrooms\", \"bathrooms\", \"floors\"\n",
    "\n",
    "Let us have a look at the columns representing the number of specific rooms. We saw that they share a floating point data type. Now we check which fractional parts arise in these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_cols = [\n",
    "    \"bedrooms\",\n",
    "    \"bathrooms\",\n",
    "    \"floors\",\n",
    "]\n",
    "\n",
    "pd.DataFrame(\n",
    "    [(col, (df_0[col] % 1).unique()) for col in room_cols],\n",
    "    columns=[\"Room Column\", \"Fractional Parts\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, it is fine to leave these columns as they are (although the distinction between values like 0.5 and 0.75 bathrooms is likely not precise ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"yr_renovated\"\n",
    "\n",
    "We saw that \"yr_renovated\" has dtype `float64`. When inspecting its unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0[\"yr_renovated\"].unique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two issue can be detected:\n",
    "\n",
    "- Floating point numbers are not necessary, integers suffice.\n",
    "- Among the values, we find `nan` and `0`. It is unclear what `0` represents: It could represent \"no renovation\" or an alternative encoding for `nan`. We will treat `0` as equivalent to `nan`.\n",
    "\n",
    "Let us address these problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0[\"yr_renovated\"]\n",
    "df_1[\"yr_renovated\"] = df_1[\"yr_renovated\"].replace(0.0, np.nan)\n",
    "# To retain NaN values, \"Int64\" instead of \"int64\" must be used.\n",
    "df_1[\"yr_renovated\"] = df_1[\"yr_renovated\"].astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"waterfront\", \"view\"\n",
    "\n",
    "It is very likely that the columns \"waterfront\" and \"view\" are, in actual fact, categorical (or even Boolean), meaning floating point numbers are unnecessary. Their unique values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_cat_cols = [\n",
    "    \"waterfront\",\n",
    "    \"view\",\n",
    "]\n",
    "\n",
    "pd.DataFrame(\n",
    "    [(col, df_0[col].unique()) for col in maybe_cat_cols],\n",
    "    columns=[\"Maybe Categorical Column\", \"Unique Values\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the columns are categorical (\"waterfront\" is even Boolean). Thus, we convert these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in maybe_cat_cols:\n",
    "    # Conversion to \"int64\" will not work since NaN values are present,\n",
    "    # we have to use \"Int64\".\n",
    "    df_1[col] = df_0[col].astype(\"Int64\")\n",
    "\n",
    "df_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ad \"sqft_*\"\n",
    "\n",
    "Finally, at Miss Williams's request, we convert columns representing square footage (those matching \"sqft_*\") into square metre (she advocates for metric units!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ratio of square metre to square foot.\n",
    "SQ_M_TO_SQ_F_RATIO = 0.09290304\n",
    "\n",
    "# The column sqft_basement has dtype object, \n",
    "# we convert it to integer (and interpret missing values as 0).\n",
    "df_0[\"sqft_basement\"] = df_0[\"sqft_basement\"].replace(\"?\", np.nan)\n",
    "df_0[\"sqft_basement\"] = pd.to_numeric(df_0[\"sqft_basement\"], errors=\"coerce\")\n",
    "df_0[\"sqft_basement\"] = df_0[\"sqft_basement\"].fillna(0).astype(\"int64\")\n",
    "\n",
    "sqft_cols = [col for col in df_0.columns if col.startswith(\"sqft_\")]\n",
    "\n",
    "for col in sqft_cols:\n",
    "    # Drop sqft column.\n",
    "    df_1 = df_1.drop(columns=[col], errors=\"ignore\")\n",
    "    # Add sqm column.\n",
    "    col_new = col.replace(\"sqft_\", \"sqm_\")\n",
    "    df_1[col_new] = SQ_M_TO_SQ_F_RATIO * df_0[col]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm Up: Three Haphazard Hypotheses on the Data\n",
    "\n",
    "To become acquainted with the data set, we formulate three arbitrary (not well-thought-out) hypotheses and check their consistency with the data.\n",
    "\n",
    "**CAVEAT:** We do *not* perform hypothesis testing in a (statistically) rigorous sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis #1: Dependence of Two Discrete Variables\n",
    "\n",
    "**Hypothesis:** The correlation between \"bathrooms\" and \"bedrooms\" is strictly positive (particularly, they are dependent).\n",
    "\n",
    "**Consistency Check:** Let us create a scatterplot with \"bathrooms\" as the x- and \"bedrooms\" as the y-variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    df_1[\"bathrooms\"], \n",
    "    df_1[\"bedrooms\"], \n",
    "    c=\"blue\", \n",
    "    alpha=0.005, \n",
    "    label=\"Location of Houses\"\n",
    ")\n",
    "\n",
    "plt.title(\"Bathrooms vs. Bedrooms\")\n",
    "plt.xlabel(\"bathrooms\")\n",
    "plt.ylabel(\"bedrooms\")\n",
    "plt.xlim(0,6)\n",
    "plt.ylim(0,8)\n",
    "\n",
    "plt.savefig(\"./plots/bathrooms-vs-bedrooms.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation tends to be slightly positive. Hence, the hypothesis should be *accepted*.\n",
    "\n",
    "**REMARK:** The following computation further corroborates the hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"bathrooms\"].corr(df_1[\"bedrooms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis #2: Dependence of a Continuous and a Discrete Variable \n",
    "\n",
    "**Hypothesis:** The variables \"price\" and \"waterfront\" are not independent.\n",
    "\n",
    "**Consistency Check:** Let us compare the distributions of \"price\" with the distribution of \"price\" conditioned on the event that \"waterfront\" equals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax[0].hist(df_1[\"price\"], bins=100, density=True)\n",
    "ax[0].set_title(\"Price\")\n",
    "ax[0].set_xlabel(\"Price\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "is_wf_1 = (df_1[\"waterfront\"] == 1) \n",
    "ax[1].hist(df_1[\"price\"][is_wf_1], bins=100, density=True)\n",
    "ax[1].set_title(\"Price | Waterfront == 1\")\n",
    "ax[1].set_xlabel(\"Price\")\n",
    "ax[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.savefig(\"./plots/price-on-waterfront.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Obviously, the two distributions are distinct. Thus, the hypotheses should be *accepted*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis #3: Dependence of Two Continuous Variables\n",
    "\n",
    "**Hypotheses:** The variables \"lat\" and \"long\" are not correlated.\n",
    "\n",
    "**Consistency Check:** Let us look at the scatterplot of \"lat\" versus \"long\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    df_1[\"lat\"], \n",
    "    df_1[\"long\"], \n",
    "    c=\"blue\", \n",
    "    alpha=0.1, \n",
    "    label=\"Location of Houses\"\n",
    ")\n",
    "\n",
    "plt.title(\"Location of Houses\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Longitude\")\n",
    "\n",
    "plt.savefig(\"./plots/lat-vs-long.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visual evidence against the hypothesis is insufficient, so it should be **accepted**.\n",
    "\n",
    "**REMARK:** The following computation also shows that the correlation deviates not drastically from zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"lat\"].corr(df_1[\"long\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "Recall that our client's description (which she and her lawyer find partly denigrating!) was as follows:\n",
    "\n",
    "> Amy Williams | Seller | Mafiosa, sells several central houses (top 10%) over time, needs average outskirt houses over time to hide from the FBI.\n",
    "\n",
    "By detokenising her requirements, we see that we have to answer the following questions:\n",
    "\n",
    "1. When is a house considered central or on the outskirts (We need a measure of **centrality**)?\n",
    "2. What does \"top 10%\" refer to: (Non-)Peripherality, price, size, ...? \n",
    "3. When is an (outskirt) house considered average (we need a measure of **exceptionality**)?\n",
    "4. When is an (outskirt) house suitable for avoiding inconveniences with the FBI? (we need a measure of ... **privacy**)?\n",
    "\n",
    "There are definitively no unique answers to these question. We have to make educated guesses (based on some plots) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad (Top 10%) \"Centrality\"\n",
    "\n",
    "We attack the first two question simultaneously.\n",
    "\n",
    "As a first approximation, we will ignore matters of curvatures and consider earth as (locally) flat. We will treat latitude and longitude as if they were lengths, i.e. as x- and y-coordinates (we are not in Arctic zones). The house locations are distributed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a measure of centrality for a house, we take the number of houses that lie in a \"rectangular\" vicinity of the house under consideration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are guesses based on the plot.\n",
    "# Of course, one can imagine more sophisticated methods to\n",
    "# determine these quantities ...\n",
    "DIST_X = 0.1\n",
    "DIST_Y = 0.1\n",
    "\n",
    "# Matrices of lateral and longitudinal distances\n",
    "# (kudos to NumPy's broadcasting magic).\n",
    "lat_diff = np.abs(df_1[\"lat\"].values[:, np.newaxis] - df_1[\"lat\"].values)\n",
    "long_diff = np.abs(df_1[\"long\"].values[:, np.newaxis] - df_1[\"long\"].values)\n",
    "inside_rectangle = (lat_diff <= DIST_X) & (long_diff <= DIST_Y)\n",
    "\n",
    "# Sum along the columns to get the number of neighbours.\n",
    "df_1[\"centrality\"] = inside_rectangle.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a histogram for the distribution of centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    df_1[\"centrality\"], \n",
    "    bins=128, \n",
    "    alpha=0.75, \n",
    "    density=True,\n",
    "    color=\"blue\", \n",
    "    edgecolor=\"blue\",\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Centrality\")\n",
    "plt.xlabel(\"Centrality\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.savefig(\"./plots/centrality-hist.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show how the centrality reflects in a scatterplot for latitude and longitude in moment.\n",
    "\n",
    "But before then, let us give a precise meaning to *top 10% central house* and *house on the outskirts*:\n",
    "\n",
    "- Central houses (top 10%): Houses with centrality above the 0.9-quantile. \n",
    "- Houses on the outskirts: Houses with centrality below the 0.1-quantile.\n",
    "\n",
    "Now, we can visualise the groups of houses on a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_10pc = df_1[\"centrality\"].quantile(0.1)\n",
    "centrality_90pc = df_1[\"centrality\"].quantile(0.9)\n",
    "\n",
    "colours = np.where(\n",
    "    df_1[\"centrality\"] > centrality_90pc, \n",
    "    \"green\",\n",
    "    np.where(\n",
    "        df_1[\"centrality\"] < centrality_10pc, \n",
    "        \"red\", \n",
    "        \"blue\",\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    df_1[\"lat\"], \n",
    "    df_1[\"long\"], \n",
    "    c=colours, \n",
    "    alpha=0.1, \n",
    "    label=\"Location of Houses\"\n",
    ")\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor='green', edgecolor='g', label=\"Top 10\"),\n",
    "    Patch(facecolor='red', edgecolor='r', label=\"Middle\"),\n",
    "    Patch(facecolor='blue', edgecolor='b', label=\"Bottom 10\"),\n",
    "]\n",
    "\n",
    "# ENIGMA How do the arguments of legend work?\n",
    "plt.legend(\n",
    "    handles=legend_elements, \n",
    "    loc=\"lower right\", \n",
    "    bbox_to_anchor=(1.33,0),\n",
    ")\n",
    "\n",
    "plt.title(\"Location of Houses, Coloured by Centrality\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Longitude\")\n",
    "\n",
    "plt.savefig(\"./plots/centrality-plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"Exceptionality\" (on the Outskirts)\n",
    "\n",
    "Gratefully, the data frame includes information on the neighbourhood mean for living space (\"sqm_living15\") and lot size (\"sqm_lot15\"). To measure exceptionality, we sum the absolute differences from these neighbourhood means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"exceptionality\"] = (\n",
    "    + (df_1[\"sqm_living\"] - df_1[\"sqm_living15\"]).abs()\n",
    "    + (df_1[\"sqm_lot\"] - df_1[\"sqm_lot15\"]).abs()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, exceptionality is distributed on the map as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"exceptionality_q\"] = pd.qcut(df_1[\"exceptionality\"], q=4, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "colours = {\"Q1\": \"green\", \"Q2\": \"yellow\", \"Q3\": \"orange\", \"Q4\": \"red\"}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=True, sharey=True)\n",
    "axes = axes.flatten()  \n",
    "\n",
    "for i, quartile in enumerate([\"Q1\", \"Q2\", \"Q3\", \"Q4\"]):\n",
    "    ax = axes[i]\n",
    "    subset = df_1[df_1[\"exceptionality_q\"] == quartile]\n",
    "    ax.scatter(df_1[\"lat\"], df_1[\"long\"], color=\"gray\", alpha=0.1, label=quartile)\n",
    "    ax.scatter(subset[\"lat\"], subset[\"long\"], color=colours[quartile], alpha=0.1, label=quartile)\n",
    "    ax.set_title(f\"Exceptionality, {quartile}\")\n",
    "    ax.set_xlabel(\"Latitude\")\n",
    "    ax.set_ylabel(\"Longitude\")\n",
    "    # ax.legend()\n",
    "\n",
    "fig.suptitle(\"Location of Houses, Coloured by Exceptionality\", fontsize=16)\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "plt.savefig(\"./plots/normality-plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no significant visible differences\n",
    "\n",
    "*The (dis-)recommended regions will be indicated by the cursor.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"Privacy\"\n",
    "\n",
    "For privacy, we consider the size of the basement (\"sqm_basement\") as a proxy measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"privacy\"] = df_1[\"sqm_basement\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we interpreted `nan` value for the column \"sqm_basement\" as `0`. For the sake of simplicity, let us simply visualise where \"privacy\" is (not) strictly positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_priv0 = df_1[df_1[\"privacy\"] == 0]\n",
    "df_priv1 = df_1[df_1[\"privacy\"] > 0]\n",
    "\n",
    "df_1[\"exceptionality_q\"] = pd.qcut(df_1[\"exceptionality\"], q=4, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "colours = {\"Q1\": \"green\", \"Q2\": \"yellow\", \"Q3\": \"orange\", \"Q4\": \"red\"}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n",
    "\n",
    "axes[0].scatter(df_1[\"lat\"], df_1[\"long\"], color=\"gray\", alpha=0.1, label=quartile)\n",
    "axes[0].scatter(df_priv0[\"lat\"], df_priv0[\"long\"], color=\"red\", alpha=0.01, label=quartile)\n",
    "axes[0].set_title(\"Without Privacy\")\n",
    "axes[0].set_xlabel(\"Latitude\")\n",
    "axes[0].set_ylabel(\"Longitude\")\n",
    "\n",
    "axes[1].scatter(df_1[\"lat\"], df_1[\"long\"], color=\"gray\", alpha=0.1, label=quartile)\n",
    "axes[1].scatter(df_priv1[\"lat\"], df_priv1[\"long\"], color=\"green\", alpha=0.01, label=quartile)\n",
    "axes[1].set_title(\"With Privacy\")\n",
    "axes[1].set_xlabel(\"Latitude\")\n",
    "axes[1].set_ylabel(\"Longitude\")\n",
    "\n",
    "fig.suptitle(\"Location of Houses, With And Without Privacy\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "plt.savefig(\"./plots/privacy-plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The (dis-)recommended regions will be indicated by the cursor.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
