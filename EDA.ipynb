{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS EDA Project ⟡ King County House Prices ⟡ *Amy Williams* Edition\n",
    "\n",
    "## Outline of the Problem\n",
    "\n",
    "<!-- | Amy Williams        | Seller      | Mafiosi, sells several central houses(top10%) over time, needs average outskirt houses over time to hide from the FBI    -->\n",
    "<!-- \n",
    "Amy Williams, businesswoman by trade (very upset that her very smart tricks are construed as organised crime and ... )  \n",
    "- wants to sells several central houses (top 10%) over time, \n",
    "- needs average outskirt houses over time to hide from the FBI -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usual Setup Tasks\n",
    "\n",
    "For analysing the data set, we first load the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the data under consideration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the csv file containing the data.\n",
    "DATAPATH = \"./data/King_County_House_prices_dataset.csv\"\n",
    "\n",
    "df_0 = pd.read_csv(DATAPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Inspection and Data Cleaning\n",
    "\n",
    "For a bird's eye-view on this data set, let us do the usual steps of inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 10 rows of the table.\n",
    "df_0.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about column dtypes and NaN values. \n",
    "df_0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common statistical quantities for the numerical columns.\n",
    "df_0.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the necessary cleaning tasks. For convenience, let us create a deep copy of the original data which will incorporate the edits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fix the data types for some columns where it is not already appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"date\"\n",
    "\n",
    "As we saw, the displayed `dtype` of \"date\" is `object`. \n",
    "\n",
    "It is always wise to use any help that is provided for working with dates which means in the context of pandas to cast the affected columns into the datetime type. \n",
    "\n",
    "To our luck, there are no further intricacies (like exotic date formats) so that the following command is sufficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"date\"] = pd.to_datetime(df_0[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"bedrooms\", \"bathrooms\", \"floors\"\n",
    "\n",
    "Let us have a look whether it is reasonable that the columns encoding the number of specific rooms really requires floating point numbers. For this, we check which fractional parts arise in these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_cols = [\n",
    "    \"bedrooms\",\n",
    "    \"bathrooms\",\n",
    "    \"floors\",\n",
    "]\n",
    "\n",
    "pd.DataFrame(\n",
    "    [(col, (df_0[col] % 1).unique()) for col in room_cols],\n",
    "    columns=[\"Room Column\", \"Fractional Parts\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it is perfectly fine to leave these columns as they are (although the discriminating criterion between 0.5 bathrooms and 0.75 bathrooms is almost surely not sharp ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"yr_renovated\"\n",
    "\n",
    "We saw that \"yr_renovated\" has dtype `float64`. When we inspect the values of this column by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0[\"yr_renovated\"].unique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", we immediately recognise three aspects:\n",
    "\n",
    "- It is very likely that the years are 10-times to high.\n",
    "- Floating point numbers are not necessary, integers are sufficient.\n",
    "- Among the values, there are `nan` and `0`. It is not definitively clear what `0` should mean: It could mean no renovation but it could also be an alternative encoding for `nan`. We will stick to the second interpretation. \n",
    "\n",
    "Let us address these problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"yr_renovated\"] = (df_0[\"yr_renovated\"]/10).map(\n",
    "    lambda x: x if x > 0 else np.nan\n",
    "# To retain NaN values, \"Int64\" instead of \"int64\" must be used.\n",
    ").astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"waterfront\", \"view\"\n",
    "\n",
    "It is very likely that the columns \"waterfront\" and \"view\" are, in actual fact, categorical (or even Boolean) so that floating point numbers are not necessary. Indeed, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_cat_cols = [\n",
    "    \"waterfront\",\n",
    "    \"view\",\n",
    "]\n",
    "\n",
    "pd.DataFrame(\n",
    "    [(col, df_0[col].unique()) for col in maybe_cat_cols],\n",
    "    columns=[\"Maybe Categorical Column\", \"Unique Values\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, they are categorical (\"waterfront\" is even Boolean). So let us convert these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in maybe_cat_cols:\n",
    "    # Conversion to \"int64\" will not work since NaN values are present,\n",
    "    # we have to use \"Int64\".\n",
    "    df_1[col] = df_0[col].astype(\"Int64\")\n",
    "\n",
    "df_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ad \"sqft_*\"\n",
    "\n",
    "As a last step, we follow a wish of Miss Williams and convert columns whose dimension is square foot (those matching \"sqft_*\") into square metre (She is an advocate of metric units!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ratio of square metre to square foot.\n",
    "SQ_M_TO_SQ_F_RATIO = 0.09290304\n",
    "\n",
    "sqft_cols = [col for col in df_0.columns if col.startswith(\"sqft_\")]\n",
    "\n",
    "for col in sqft_cols:\n",
    "    # Dropping sqft column.\n",
    "    df_1 = df_1.drop(columns=[col], errors=\"ignore\")\n",
    "    # Adding sqm column.\n",
    "    col_new = col.replace(\"sqft_\", \"sqm_\")\n",
    "    df_1[col_new] = SQ_M_TO_SQ_F_RATIO * df_0[col]\n",
    "\n",
    "df_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm Up: Three Haphazard Hypotheses on the Data\n",
    "\n",
    "To get some acquaintance with the data set, we state three arbitrary (not well-thought-out) hypotheses on the given data and check their consistency with the data afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis #1: Dependence of Two Discrete Variables\n",
    "\n",
    "**Hypothesis:** The correlation between \"bathrooms\" and \"bedrooms\" is strictly positive (particularly, they are dependent).\n",
    "\n",
    "**Consistency Check:** Let us draw a scatterplot with \"bathrooms\" as x- and \"bedrooms\" as y-variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    df_1[\"bathrooms\"], \n",
    "    df_1[\"bedrooms\"], \n",
    "    c=\"blue\", \n",
    "    alpha=0.005, \n",
    "    label=\"Location of Houses\"\n",
    ")\n",
    "\n",
    "plt.title(\"Location of Houses\")\n",
    "plt.xlabel(\"bathrooms\")\n",
    "plt.ylabel(\"bedrooms\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly positive correlation can be recognised. Hence, the hypothesis should be *accepted*.\n",
    "\n",
    "**REMARK:** The following computation corroborates the hypothesis, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"bathrooms\"].corr(df_1[\"bedrooms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis #2: Dependence of a Continuous and a Discrete Variable \n",
    "\n",
    "**Hypothesis:** The variables \"price\" and \"waterfront\" are not independent.\n",
    "\n",
    "**Consistency Check:** Let us compare the distributions of \"price\" and the distribution of \"price\" conditioned on the event that \"waterfront\" is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax[0].hist(df_1[\"price\"], bins=100, density=True)\n",
    "ax[0].set_title(\"Price\")\n",
    "ax[0].set_xlabel(\"Price\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "is_wf_1 = (df_1[\"waterfront\"] == 1) \n",
    "ax[1].hist(df_1[\"price\"][is_wf_1], bins=100, density=True)\n",
    "ax[1].set_title(\"Price | Waterfront == 1\")\n",
    "ax[1].set_xlabel(\"Price\")\n",
    "ax[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Obviously, these distributions are very different. The hypotheses should be *accepted*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis #3: Dependence of Two Continuous Variables\n",
    "\n",
    "**Hypotheses** \"lat\" and \"long\" are not correlated.\n",
    "\n",
    "**Consistency Check:** Let us look at the scatterplot of \"lat\" and \"long\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    df_1[\"lat\"], \n",
    "    df_1[\"long\"], \n",
    "    c=\"blue\", \n",
    "    alpha=0.1, \n",
    "    label=\"Location of Houses\"\n",
    ")\n",
    "\n",
    "plt.title(\"Location of Houses\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Longitude\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not enough visual evidence against the hypotheses, it should be **accepted**.\n",
    "\n",
    "**REMARK:** The following computation also shows that the correlation deviates not drastically from zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"lat\"].corr(df_1[\"long\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "Recall that our client's description (which is partially denigrating in her and her lawyer's opinion!) was as follows:\n",
    "\n",
    "> Amy Williams | Seller | Mafiosa, sells several central houses (top 10%) over time, needs average outskirt houses over time to hide from the FBI.\n",
    "\n",
    "By detokenising the demands of our client, we see that we have to answer the following questions:\n",
    "\n",
    "1. When is a house considered central or on the outskirts? I.e., we need here an appropriate measure for **centrality**.\n",
    "2. What does \"top 10%\" refer to: To (non-)peripherality, price, size, ...? I.e., we have to attribute \"top 10%\" to some appropriate measure.\n",
    "3. When is an (outskirt) house considered average? I.e., we need here an appropriate measure for **normality**.\n",
    "4. When is an (outskirt) house suitable for avoiding inconveniences with the FBI? I.e., we need here an appropriate for ... **privacy**.\n",
    "\n",
    "There are definitively no unique answers to these question. We have to make educated guesses (based on some plots) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad (Top 10%) \"Centrality\"\n",
    "\n",
    "We attack the first two question simultaneously.\n",
    "\n",
    "As a first approximation, we will ignore matters of curvatures and consider earth as (locally) flat. We will treat latitude and longitude as if they were lengths, i.e. as x- and y-coordinate (we are not in Arctic zones). The distribution of the house locations is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a measure of centrality for a house, we take the number of houses that lie in a \"rectangular\" vicinity of the original house. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are guesses based on the plot on how to choose\n",
    "# the side lengths of \n",
    "# Of course, one can imagine more sophisticated methods to\n",
    "# determine these quantities.\n",
    "DIST_X = 0.1\n",
    "DIST_Y = 0.1\n",
    "\n",
    "# Matrices of lateral and longitudinal distances\n",
    "# (kudos to NumPy's broadcasting magic).\n",
    "lat_diff = np.abs(df_1[\"lat\"].values[:, np.newaxis] - df_1[\"lat\"].values)\n",
    "long_diff = np.abs(df_1[\"long\"].values[:, np.newaxis] - df_1[\"long\"].values)\n",
    "inside_rectangle = (lat_diff <= DIST_X) & (long_diff <= DIST_Y)\n",
    "\n",
    "# Sum along the columns to get the number of neighbours.\n",
    "df_1[\"centrality\"] = inside_rectangle.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a histogram for the distribution of centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    df_1[\"centrality\"], \n",
    "    bins=128, \n",
    "    alpha=0.75, \n",
    "    density=True,\n",
    "    color=\"blue\", \n",
    "    edgecolor=\"blue\",\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Centrality\")\n",
    "plt.xlabel(\"Centrality\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show how the centrality reflects in a scatterplot for latitude and longitude in moment.\n",
    "\n",
    "But before then, let us give a precise meaning to top 10% central house and house on the outskirts. \n",
    "We will simply define that \n",
    "\n",
    "- central houses (top 10%) are those houses that lie above the 0.9-quantile with respect to centrality.\n",
    "- houses on the outskirt are those houses that lie below the 0.1-quantile with respect to centrality.\n",
    "\n",
    "Now we are able to visualise central and peripheral houses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_10pc_threshold = df_1[\"centrality\"].quantile(0.1)\n",
    "top_10pc_threshold = df_1[\"centrality\"].quantile(0.9)\n",
    "\n",
    "colours = np.where(\n",
    "    df_1[\"centrality\"] > top_10pc_threshold, \n",
    "    \"green\",\n",
    "    np.where(\n",
    "        df_1[\"centrality\"] < bottom_10pc_threshold, \n",
    "        'red', \n",
    "        'blue',\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    df_1[\"lat\"], \n",
    "    df_1[\"long\"], \n",
    "    c=colours, \n",
    "    alpha=0.1, \n",
    "    label=\"Location of Houses\"\n",
    ")\n",
    "\n",
    "# TODO Replace this ugly ad-hoc solution.\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor='green', edgecolor='g', label=\"Top 10\"),\n",
    "    Patch(facecolor='red', edgecolor='r', label=\"Middle\"),\n",
    "    Patch(facecolor='blue', edgecolor='b', label=\"Bottom 10\"),\n",
    "]\n",
    "\n",
    "# ENIGMA How do the arguments of legend work?\n",
    "plt.legend(handles=legend_elements, loc=\"lower right\", bbox_to_anchor=(1.33,0))\n",
    "\n",
    "plt.title(\"Location of Houses, Coloured by Centrality\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Longitude\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"Normality\" (on the Outskirts)\n",
    "\n",
    "Gratefully, the data frame contains two columns which carry information about some neighbourhood's mean with respect to the nearest neighbourhood, namely \"sqm_living15\" and \"sqm_lot15\". As normality, we take the sum of the distances that a house has to its neighbourhood means, more precisely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"normality\"] = (\n",
    "    + (df_1[\"sqm_living\"] - df_1[\"sqm_living15\"]).abs()\n",
    "    + (df_1[\"sqm_lot\"] - df_1[\"sqm_lot15\"]).abs()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"Privacy\"\n",
    "\n",
    "Lock herself at the basement. So as a measure of privacy we choose simply \"sqm_basement\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"privacy\"] = df_1[\"sqm_basement\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
