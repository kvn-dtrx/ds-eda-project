{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6006171f",
   "metadata": {},
   "source": [
    "# Data Retrieval\n",
    "\n",
    "For the sake of completeness, we shall demonstrate which type of data may be downloaded from a given database. Naturally, credentials for access to the database remain requisite.\n",
    "\n",
    "The following modules shall be utilised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, select, inspect, text\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191764c8",
   "metadata": {},
   "source": [
    "Furthermore, we shall define the following environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f64a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the root directory of the repo.\n",
    "root_dir_ = subprocess.check_output(\n",
    "    [\"git\", \"rev-parse\", \"--show-toplevel\"],\n",
    "    text=True,\n",
    ")\n",
    "ROOT_DIR = root_dir_.strip()\n",
    "\n",
    "# Path to the data directory.\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "BKP_DIR = os.path.join(DATA_DIR, \"bkp\")\n",
    "PARQUET_DIR = os.path.join(BKP_DIR, \"data\")\n",
    "META_DIR = os.path.join(BKP_DIR, \"metadata\")\n",
    "DDL_DIR = os.path.join(BKP_DIR, \"ddl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f1e55",
   "metadata": {},
   "source": [
    "As an additional preparatory step, we may create the subdirectories designated for storing our downloads and define auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b6971",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "os.makedirs(META_DIR, exist_ok=True)\n",
    "os.makedirs(DDL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462140fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_filename(schema: str, table: str, extension: str) -> str:\n",
    "    core = f\"{schema}.{table}\".replace(\".\", \"_\")\n",
    "    filename = f\"{core}.{extension}\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9628b8",
   "metadata": {},
   "source": [
    "We load the credentials required for database access from the `.env` file into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd43fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"scheme\": os.getenv(\"SCHEME\"),\n",
    "    \"database\": os.getenv(\"DATABASE\"),\n",
    "    \"user\": os.getenv(\"USER\"),\n",
    "    \"password\": os.getenv(\"PASSWORD\"),\n",
    "    \"host\": os.getenv(\"HOST\"),\n",
    "    \"port\": os.getenv(\"PORT\")\n",
    "}\n",
    "\n",
    "DB_URI = (\n",
    "    \"{scheme}://{user}:{password}@{host}:{port}/{database}\"\n",
    "    .format(**DB_CONFIG)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d552880",
   "metadata": {},
   "source": [
    "We proceed to establish a connection with the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d1bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = create_engine(DB_URI)\n",
    "inspector = inspect(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c710b",
   "metadata": {},
   "source": [
    "This step concludes our preparations, and we are now ready to commence the retrieval of various data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf15e1",
   "metadata": {},
   "source": [
    "## Schemas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_query = \"\"\"\n",
    "    SELECT schema_name \n",
    "        FROM information_schema.schemata\n",
    "        WHERE schema_name NOT IN ('pg_catalog', 'information_schema')\n",
    "\"\"\"\n",
    "\n",
    "with db.connect() as conn:\n",
    "    result = conn.execute(text(schema_query))\n",
    "    schemas = [row[0] for row in result]\n",
    "\n",
    "print(\"Schemas found:\")\n",
    "for schema in schemas:\n",
    "    print(f\"  - {schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19710774",
   "metadata": {},
   "source": [
    "## Tables, Columns, PKs, FKs, Indexes, Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b4e51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_inventory = {}\n",
    "\n",
    "for schema in schemas:\n",
    "    schema_inventory[schema] = []\n",
    "    metadata = MetaData()\n",
    "    try:\n",
    "        metadata.reflect(bind=db, schema=schema)\n",
    "    except Exception as e:\n",
    "        print(f\"Schema {schema} reflection failed:\")\n",
    "        print(f\"  {e}\")\n",
    "        continue\n",
    "\n",
    "    for full_name, table in metadata.tables.items():\n",
    "        table_name = table.name\n",
    "        fq_name = f\"{schema}.{table_name}\"\n",
    "        schema_inventory[schema].append(table_name)\n",
    "        print(f\"üì¶ Exporting {fq_name}...\")\n",
    "\n",
    "        try:\n",
    "            with db.connect() as conn:\n",
    "                df = pd.read_sql(select(table), conn)\n",
    "                df.to_parquet(os.path.join(PARQUET_DIR, safe_filename(schema, table_name, \"parquet\")), compression=\"snappy\")\n",
    "\n",
    "                columns = inspector.get_columns(table_name, schema=schema)\n",
    "                pk = inspector.get_pk_constraint(table_name, schema=schema)\n",
    "                fks = inspector.get_foreign_keys(table_name, schema=schema)\n",
    "                indexes = inspector.get_indexes(table_name, schema=schema)\n",
    "\n",
    "                metadata_info = {\n",
    "                    \"schema\": schema,\n",
    "                    \"table_name\": table_name,\n",
    "                    \"columns\": columns,\n",
    "                    \"primary_key\": pk,\n",
    "                    \"foreign_keys\": fks,\n",
    "                    \"indexes\": indexes,\n",
    "                }\n",
    "\n",
    "\n",
    "                flat_metadata_info = {\n",
    "                    k: v if not isinstance(v, (list, dict)) else json.dumps(v, default=str)\n",
    "                    for k, v in metadata_info.items()\n",
    "                }\n",
    "\n",
    "                meta_df = pd.DataFrame([flat_metadata_info])\n",
    "                meta_df.to_parquet(\n",
    "                    os.path.join(META_DIR, safe_filename(schema, table_name, \"parquet\")),\n",
    "                    engine=\"pyarrow\",\n",
    "                    compression=\"snappy\"\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    ddl_result = conn.execute(text(f\"SELECT pg_get_tabledef('{schema}.{table_name}'::regclass)\"))\n",
    "                    ddl = ddl_result.scalar()\n",
    "                    if ddl:\n",
    "                        with open(os.path.join(DDL_DIR, safe_filename(schema, table_name, \"sql\")), \"w\") as f:\n",
    "                            f.write(ddl)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to export {fq_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3220b9",
   "metadata": {},
   "source": [
    "## (Materialised) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c06325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "views_query = \"\"\"\n",
    "    SELECT schemaname, viewname, definition\n",
    "        FROM pg_views\n",
    "        WHERE schemaname NOT IN ('pg_catalog', 'information_schema')\n",
    "\"\"\"\n",
    "\n",
    "matviews_query = \"\"\"\n",
    "    SELECT schemaname, matviewname \n",
    "        AS viewname, definition\n",
    "        FROM pg_matviews\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with db.connect() as conn:\n",
    "    views = conn.execute(text(views_query)).fetchall()\n",
    "    matviews = conn.execute(text(matviews_query)).fetchall()\n",
    "    for view in views + matviews:\n",
    "        s, v, d = view\n",
    "        path = os.path.join(DDL_DIR, safe_filename(s, v, \"view.sql\"))\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea536ab",
   "metadata": {},
   "source": [
    "## Functions and Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs_query = \"\"\"\n",
    "    SELECT n.nspname AS schema, p.proname AS name, pg_get_functiondef(p.oid) AS definition\n",
    "        FROM pg_proc p\n",
    "        JOIN pg_namespace n \n",
    "        ON p.pronamespace = n.oid\n",
    "        WHERE n.nspname NOT IN ('pg_catalog', 'information_schema')\n",
    "\"\"\"\n",
    "\n",
    "with db.connect() as conn:\n",
    "    funcs = conn.execute(text(funcs_query)).fetchall()\n",
    "    for row in funcs:\n",
    "        schema, table, definition = row\n",
    "        fname = os.path.join(DDL_DIR, safe_filename(schema, table, \"function.sql\"))\n",
    "        with open(fname, \"w\") as f:\n",
    "            f.write(definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd54bc",
   "metadata": {},
   "source": [
    "## Triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "triggers_query = \"\"\"\n",
    "    SELECT event_object_schema AS schema, event_object_table AS table, trigger_name, action_timing, event_manipulation, action_statement\n",
    "        FROM information_schema.triggers\n",
    "        ORDER BY event_object_schema, event_object_table\n",
    "\"\"\"\n",
    "\n",
    "with db.connect() as conn:\n",
    "    triggers = conn.execute(text(triggers_query)).fetchall()\n",
    "    df = pd.DataFrame([dict(row._mapping) for row in triggers])\n",
    "    df.to_parquet(os.path.join(META_DIR, \"triggers.parquet\"), compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb1e435",
   "metadata": {},
   "source": [
    "## Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c11b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_query = \"\"\"\n",
    "    SELECT sequence_schema, sequence_name, data_type, start_value, minimum_value, maximum_value, increment\n",
    "        FROM information_schema.sequences\n",
    "\"\"\"\n",
    "\n",
    "with db.connect() as conn:\n",
    "    sequences = conn.execute(text(sequences_query)).fetchall()\n",
    "    df = pd.DataFrame([dict(row._mapping) for row in sequences])\n",
    "    df.to_parquet(os.path.join(META_DIR, \"sequences.parquet\"), compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c21533",
   "metadata": {},
   "source": [
    "## Privileges and Grants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0eb03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grants_query = \"\"\"\n",
    "    SELECT grantee, table_schema, table_name, privilege_type\n",
    "    FROM information_schema.role_table_grants\n",
    "\"\"\"\n",
    "\n",
    "with db.connect() as conn:\n",
    "    grants = conn.execute(text(grants_query)).fetchall()\n",
    "    df = pd.DataFrame([dict(row._mapping) for row in grants])\n",
    "    df.to_parquet(os.path.join(META_DIR, \"grants.parquet\"), compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389e6ab",
   "metadata": {},
   "source": [
    "## Table Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_query = \"\"\"\n",
    "    SELECT schemaname, relname, n_live_tup, n_dead_tup, pg_total_relation_size(relid) AS total_bytes\n",
    "        FROM pg_stat_user_tables\n",
    "\"\"\"\n",
    "\n",
    "with db.connect() as conn:\n",
    "    stats = conn.execute(text(stats_query)).fetchall()\n",
    "    df = pd.DataFrame([dict(row._mapping) for row in stats])\n",
    "    df.to_parquet(os.path.join(META_DIR, \"table_stats.parquet\"), compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741e04a",
   "metadata": {},
   "source": [
    "## Global Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f46f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(BKP_DIR, \"schema_inventory.json\"), \"w\") as f:\n",
    "    json.dump(schema_inventory, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
