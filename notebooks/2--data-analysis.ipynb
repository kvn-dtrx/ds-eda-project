{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "## Outline of the Problem\n",
    "\n",
    "Let us cite the original description of our client: \n",
    "\n",
    "> Amy Williams (Seller, Mafiosa) sells several central houses (top $10%$) over time, needs average outskirt houses over time to hide from the FBI.\n",
    "\n",
    "As Miss Williams is somewhat distressed that her astute manoeuvres are misconstrued as organised crime, let us recast the version proposed by her solicitor:\n",
    "\n",
    "> Amy Williams, a businesswoman by trade, …\n",
    "> - wishes to sell several central houses (top $10%$) over time, and\n",
    "> - requires average outskirt houses over time to avoid unwelcome attention from the FBI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To analyse the data set, we begin with loading the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Required for some plot fine tuning.\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we employ the following environment variables. As in the preceding notebook, the variable `DF_PKL_PATH` denotes the path to the pickled data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to root directory of the repo.\n",
    "root_dir_ = subprocess.check_output(\n",
    "    [\"git\", \"rev-parse\", \"--show-toplevel\"],\n",
    "    text=True,\n",
    ")\n",
    "ROOT_DIR = root_dir_.strip()\n",
    "\n",
    "# Path to pickled dataframe.\n",
    "DF_PKL_PATH = os.path.join(ROOT_DIR, \"data\",  \"df.pkl\")\n",
    "\n",
    "# Path to plots directory.\n",
    "PLOTS_DIR = os.path.join(ROOT_DIR, \"notebooks\", \"plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two auxiliary functions; the first offers a convenient wrapper for `plt.savefig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_savefig(\n",
    "    plot_corename,\n",
    "    plot_extension = \".png\",\n",
    "    base_directory = PLOTS_DIR,\n",
    ") -> None:\n",
    "    plot_name = f\"{plot_corename}{plot_extension}\"\n",
    "    plot_path = os.path.join(base_directory, plot_name)\n",
    "    plt.savefig(plot_path, bbox_inches=\"tight\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function serves to annotate regions within plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rectangle(ax, x, y, width, height, color=\"blue\", linewidth=1):\n",
    "    circle = patches.Rectangle(\n",
    "        (x - width / 2, y - height / 2),\n",
    "        width,\n",
    "        height,\n",
    "        linewidth=linewidth,\n",
    "        edgecolor=color,\n",
    "        facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final setup step, we import the data set under consideration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(DF_PKL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Inspection and Data Cleaning\n",
    "\n",
    "As a preliminary inspection, let us invoke the info and sample methods of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment was accompanied by a table clarifying the meanings of the column headings. Let us reproduce it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Identifier | Description |\n",
    "| --- | --- |\n",
    "| id | unique identifier for a house |\n",
    "| dateDate | house was sold |\n",
    "| pricePrice | is prediction target |\n",
    "| bedroomsNumber | # of bedrooms |\n",
    "| bathroomsNumber | # of bathrooms |\n",
    "| sqft_livingsquare | footage of the home |\n",
    "| sqft_lotsquare | footage of the lot |\n",
    "| floorsTotal | floors (levels) in house |\n",
    "| waterfront | House which has a view to a waterfront |\n",
    "| view | quality of view |\n",
    "| condition | How good the condition is ( Overall ) |\n",
    "| grade | overall grade given to the housing unit, based on King County grading system |\n",
    "| sqft_above | square footage of house apart from basement |\n",
    "| sqft_basement | square footage of the basement |\n",
    "| yr_built | Built Year |\n",
    "| yr_renovated | Year when house was renovated |\n",
    "| zipcode | zip |\n",
    "| lat | Latitude coordinate |\n",
    "| long | Longitude coordinate |\n",
    "| sqft_living15 | The square footage of interior housing living space for the nearest 15 neighbors |\n",
    "| sqft_lot15 | The square footage of the land lots of the nearest 15 neighbors |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we consider the statistical properties of the numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Let us now begin with the necessary cleaning tasks. In particular, we correct the data types for several columns where necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"date\"\n",
    "\n",
    "As previously observed, the displayed `dtype` of \"date\" is `object`. Working with dates is considerably simplified if the column is cast to the appropriate type.\n",
    "\n",
    "It is always advisable to employ the tools available for working with dates. In pandas, this entails casting the relevant columns to the datetime type. \n",
    "\n",
    "Fortunately, there are no additional complications—such as unconventional date formats—so the following command shall suffice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"bedrooms\", \"bathrooms\", \"floors\"\n",
    "\n",
    "Let us examine the columns representing the number of specific rooms. We observed that they share a floating-point data type. We shall now identify which fractional parts occur in these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_cols = [\n",
    "    \"bedrooms\",\n",
    "    \"bathrooms\",\n",
    "    \"floors\",\n",
    "]\n",
    "\n",
    "pd.DataFrame(\n",
    "    [(col, (df[col] % 1).unique()) for col in room_cols],\n",
    "    columns=[\"Room Column\", \"Fractional Parts\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, it is permissible to leave these columns as they stand—although the distinction between values like 0.5 and 0.75 bathrooms appears somewhat artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"yr_renovated\"\n",
    "\n",
    "We observed that \"yr_renovated\" possesses the data type `float64`. The unique values are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"yr_renovated\"].unique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note two issues:\n",
    "\n",
    "- The use of floating-point numbers is unnecessary; integers suffice.\n",
    "- Among the values appear `nan` and `0`. The meaning of `0` remains obscure: It may denote \"no renovation\" or serve as an alternative encoding for `nan`. We shall regard `0` as equivalent to `nan`.\n",
    "\n",
    "We shall now proceed to address these matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"yr_renovated\"]\n",
    "df[\"yr_renovated\"] = df[\"yr_renovated\"].replace(0.0, np.nan)\n",
    "# To preserve NaN values, \"Int64\" instead of \"int64\" must be employed.\n",
    "df[\"yr_renovated\"] = df[\"yr_renovated\"].astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"waterfront\", \"view\"\n",
    "\n",
    "It is highly probable that the columns \"waterfront\" and \"view\" are, in truth, categorical—or even Boolean—so floating-point numbers are unnecessary. The unique values are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_cat_cols = [\n",
    "    \"waterfront\",\n",
    "    \"view\",\n",
    "]\n",
    "\n",
    "pd.DataFrame(\n",
    "    [(col, df[col].unique()) for col in maybe_cat_cols],\n",
    "    columns=[\"Maybe Categorical Column\", \"Unique Values\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the columns are categorical, and \"waterfront\" is even Boolean. Accordingly, we shall convert these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in maybe_cat_cols:\n",
    "    # Conversion to \"int64\" will not work since NaN values are present,\n",
    "    # we have to use \"Int64\".\n",
    "    df[col] = df[col].astype(\"Int64\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"sqft_*\"\n",
    "\n",
    "Finally, at Miss Williams's request, we convert the columns representing square footage (those matching \"sqft_*\") into square metres as she advocates the use of metric units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ratio of square metre to square foot.\n",
    "SQ_M_TO_SQ_F_RATIO = 0.09290304\n",
    "\n",
    "sqft_cols = [col for col in df.columns if col.startswith(\"sqft_\")]\n",
    "\n",
    "if \"sqft_basement\" in sqft_cols:\n",
    "    # The column sqft_basement has data type object.\n",
    "    # We convert it to integer and interpret missing values as 0.\n",
    "    df[\"sqft_basement\"] = df[\"sqft_basement\"].replace(\"?\", np.nan)\n",
    "    df[\"sqft_basement\"] = pd.to_numeric(df[\"sqft_basement\"], errors=\"coerce\")\n",
    "    df[\"sqft_basement\"] = df[\"sqft_basement\"].fillna(0).astype(\"int64\")\n",
    "\n",
    "for col in sqft_cols:\n",
    "    # Add sqm column.\n",
    "    col_new = col.replace(\"sqft_\", \"sqm_\")\n",
    "    df[col_new] = SQ_M_TO_SQ_F_RATIO * df[col]\n",
    "    # Drops sqft column.\n",
    "    df = df.drop(columns=[col], errors=\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Haphazard Hypotheses Regarding the Dataset\n",
    "\n",
    "To familiarise ourselves with the dataset, we shall formulate three haphazard hypotheses and examine their consistency with the data.\n",
    "\n",
    "**Caveat:** We do *not* conduct hypothesis testing in a statistically rigorous sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis #1: Dependence of Two Discrete Variables\n",
    "\n",
    "**Hypothesis:** The correlation between \"bathrooms\" and \"bedrooms\" is strictly positive. Particularly, they are dependent.\n",
    "\n",
    "**Consistency Check:** Let us produce a scatterplot, taking \"bathrooms\" as the $x$- and \"bedrooms\" as the $y$-variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    df[\"bathrooms\"], \n",
    "    df[\"bedrooms\"], \n",
    "    c=\"blue\", \n",
    "    alpha=0.005, \n",
    "    label=\"Location of Houses\"\n",
    ")\n",
    "\n",
    "plt.title(\"Bathrooms vs. Bedrooms\")\n",
    "plt.xlabel(\"bathrooms\")\n",
    "plt.ylabel(\"bedrooms\")\n",
    "plt.xlim(0,6)\n",
    "plt.ylim(0,8)\n",
    "\n",
    "plt_savefig(\"bathrooms-vs-bedrooms\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation appears to be mildly positive. Hence, the hypothesis may be *accepted*.\n",
    "\n",
    "**REMARK:** The following computation further substantiates the hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bathrooms\"].corr(df[\"bedrooms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis #2: Dependence of a Continuous and a Discrete Variable \n",
    "\n",
    "**Hypothesis:** The variables \"price\" and \"waterfront\" are not independent.\n",
    "\n",
    "**Consistency Check:** Let us compare the (unconditional) distribution of \"price\" with the distribution of \"price\" conditioned on the event that \"waterfront\" equals $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax[0].hist(df[\"price\"], bins=100, density=True)\n",
    "ax[0].set_title(\"Price\")\n",
    "ax[0].set_xlabel(\"Price\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "is_wf_1 = (df[\"waterfront\"] == 1) \n",
    "ax[1].hist(df[\"price\"][is_wf_1], bins=100, density=True)\n",
    "ax[1].set_title(\"Price | Waterfront == 1\")\n",
    "ax[1].set_xlabel(\"Price\")\n",
    "ax[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt_savefig(\"price-on-waterfront\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, the two distributions differ. Therefore, the hypothesis shall be *accepted*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis #3: Dependence of Two Continuous Variables\n",
    "\n",
    "**Hypotheses:** The variables \"lat\" and \"long\" are not correlated.\n",
    "\n",
    "**Consistency Check:** Let us examine the scatterplot of \"lat\" versus \"long\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    df[\"long\"],\n",
    "    df[\"lat\"],\n",
    "    c=\"blue\",\n",
    "    alpha=0.1,\n",
    "    label=\"Location of Houses\",\n",
    ")\n",
    "\n",
    "plt.title(\"Location of Houses\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "plt_savefig(\"long-vs-lat\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visual evidence disfavouring the hypothesis is insufficient; hence, it shall be **accepted**.\n",
    "\n",
    "**Remark:** The subsequent computation likewise demonstrates that the correlation does not deviate significantly from zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"long\"].corr(df[\"lat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "Recall our client's description—which she and her solicitor deem somewhat denigratory—was as follows:\n",
    "\n",
    "> Amy Williams | Seller | Mafiosa, sells several central houses (top $10%$) over time, needs average outskirt houses over time to hide from the FBI.\n",
    "\n",
    "By unpacking her requirements, we discern that the following questions must be addressed:\n",
    "\n",
    "1. When is a house considered central or located on the outskirts? — We require a measure of **centrality**.\n",
    "2. To what does \"top $10%$\" refer? (Non-)peripherality, price, size, or other factors?\n",
    "3. When is an (outskirt) house considered average? — We require a measure of **exceptionality**.\n",
    "4. When is an (outskirt) house suitable for evading scrutinity by the FBI? — We require a measure of **privacy**.\n",
    "\n",
    "There are certainly no unique answers to these question. We must make educated guesses based on various plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad (Top 10%) \"Centrality\"\n",
    "\n",
    "We shall address the first two questions simultaneously.\n",
    "\n",
    "As a first approximation, we shall disregard considerations of curvature and regard the Earth as (locally) flat. Latitude and longitude shall be treated as ordinary lengths, that is, as $x$- and $y$-coordinates (King County lies far from the Arctic regions).\n",
    "\n",
    "As a measure of centrality for a house, we select the number of houses that lie within a \"rectangular\" vicinity of the house under consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are guesses based on the plot.\n",
    "# Of course, one may invent more sophisticated methods to\n",
    "# determine these quantities ...\n",
    "DIST_X = 0.1\n",
    "DIST_Y = 0.1\n",
    "\n",
    "# Matrices of lateral and longitudinal distances\n",
    "# (kudos to NumPy's broadcasting magic).\n",
    "long_diff = np.abs(df[\"long\"].values[:, np.newaxis] - df[\"long\"].values)\n",
    "lat_diff = np.abs(df[\"lat\"].values[:, np.newaxis] - df[\"lat\"].values)\n",
    "inside_rectangle = (long_diff <= DIST_Y) & (lat_diff <= DIST_X)\n",
    "\n",
    "# Sum along the columns to get the number of neighbours.\n",
    "df[\"centrality\"] = inside_rectangle.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a histogram illustrating the distribution of centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    df[\"centrality\"], \n",
    "    bins=128, \n",
    "    alpha=0.75, \n",
    "    density=True,\n",
    "    color=\"blue\", \n",
    "    edgecolor=\"blue\",\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Centrality\")\n",
    "plt.xlabel(\"Centrality\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt_savefig(\"centrality-hist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presently, we shall visualise the spatial distribution of centrality across the map of King County.\n",
    "\n",
    "Before proceeding, let us assign a precise meaning to the expressions \"top $10%$ central house\" and \"house on the outskirts\":\n",
    "\n",
    "- Central houses (top 10%): Houses with centrality exceeding the $0.9$-quantile.\n",
    "- Houses on the outskirts: Houses with centrality deceeding the $0.1$-quantile.\n",
    "\n",
    "We may now viualise the respective groups of houses on a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_10pc = df[\"centrality\"].quantile(0.1)\n",
    "centrality_90pc = df[\"centrality\"].quantile(0.9)\n",
    "\n",
    "colours = np.where(\n",
    "    df[\"centrality\"] > centrality_90pc,\n",
    "    \"green\",\n",
    "    np.where(\n",
    "        df[\"centrality\"] < centrality_10pc,\n",
    "        \"red\",\n",
    "        \"blue\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "plt.scatter(df[\"long\"], df[\"lat\"], c=colours, alpha=0.1, label=\"Location of Houses\")\n",
    "\n",
    "legend_elements = [\n",
    "    patches.Patch(facecolor=\"green\", label=\"Top 10\"),\n",
    "    patches.Patch(facecolor=\"blue\", label=\"Middle\"),\n",
    "    patches.Patch(facecolor=\"red\", label=\"Bottom 10\"),\n",
    "]\n",
    "\n",
    "# ENIGMA: How do the arguments of legend work?\n",
    "plt.legend(\n",
    "    handles=legend_elements,\n",
    "    loc=\"lower right\",\n",
    "    bbox_to_anchor=(1.33, 0),\n",
    ")\n",
    "\n",
    "plt.title(\"Location of Houses, Coloured by Centrality\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "plt_savefig(\"centrality-plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"Exceptionality\" (on the Outskirts)\n",
    "\n",
    "Fortunately, the data frame includes information on the neighbourhood means for living space (\"sqm_living15\") and lot size (\"sqm_lot15\"). To measure exceptionality, we sum the absolute deviations from these neighbourhood means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"exceptionality\"] = (\n",
    "    + (df[\"sqm_living\"] - df[\"sqm_living15\"]).abs()\n",
    "    + (df[\"sqm_lot\"] - df[\"sqm_lot15\"]).abs()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot visualises exceptionality on the map. Regions we advise against are framed in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"exceptionality_q\"] = pd.qcut(df[\"exceptionality\"], q=4, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "colours = {\"Q1\": \"green\", \"Q2\": \"yellow\", \"Q3\": \"orange\", \"Q4\": \"red\"}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=True, sharey=True)\n",
    "axes = axes.flatten()  \n",
    "\n",
    "for i, quartile in enumerate([\"Q1\", \"Q2\", \"Q3\", \"Q4\"]):\n",
    "    ax = axes[i]\n",
    "    subset = df[df[\"exceptionality_q\"] == quartile]\n",
    "    ax.scatter(df[\"long\"], df[\"lat\"], color=\"gray\", alpha=0.1, label=quartile)\n",
    "    ax.scatter(subset[\"long\"], subset[\"lat\"], color=colours[quartile], alpha=0.1, label=quartile)\n",
    "    ax.set_title(f\"Exceptionality, {quartile}\")\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    # ax.legend()\n",
    "\n",
    "fig.suptitle(\"Location of Houses, Coloured by Exceptionality\", fontsize=16)\n",
    "\n",
    "# Adjusts layout to prevent overlapping.\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "draw_rectangle(axes[0], x=-122.00, y=47.25, width=0.10, height=0.05, color=\"blue\")\n",
    "draw_rectangle(axes[0], x=-122.45, y=47.40, width=0.10, height=0.05, color=\"blue\")\n",
    "draw_rectangle(axes[0], x=-122.00, y=47.42, width=0.10, height=0.05, color=\"blue\")\n",
    "draw_rectangle(axes[0], x=-122.10, y=47.76, width=0.10, height=0.05, color=\"blue\")\n",
    "\n",
    "plt_savefig(\"exceptionality-plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad \"Privacy\"\n",
    "\n",
    "As a proxy for privacy, we consider the size of the basement (\"sqm_basement\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"privacy\"] = df[\"sqm_basement\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we interpreted the `nan` values in the column \"sqm_basement\" as `0`. For the sake of simplicity, let us visualise solely where \"privacy\" is, or is not, strictly positive. Regions we advise against are framed in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_priv0 = df[df[\"privacy\"] == 0]\n",
    "df_priv1 = df[df[\"privacy\"] > 0]\n",
    "\n",
    "df[\"exceptionality_q\"] = pd.qcut(df[\"exceptionality\"], q=4, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "colours = {\"Q1\": \"green\", \"Q2\": \"yellow\", \"Q3\": \"orange\", \"Q4\": \"red\"}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n",
    "\n",
    "axes[0].scatter(df[\"long\"], df[\"lat\"], color=\"gray\", alpha=0.1, label=quartile)\n",
    "axes[0].scatter(df_priv0[\"long\"], df_priv0[\"lat\"], color=\"red\", alpha=0.01, label=quartile)\n",
    "axes[0].set_title(\"Without Privacy\")\n",
    "axes[0].set_xlabel(\"Longitude\")\n",
    "axes[0].set_ylabel(\"Latitude\")\n",
    "\n",
    "axes[1].scatter(df[\"long\"], df[\"lat\"], color=\"gray\", alpha=0.1, label=quartile)\n",
    "axes[1].scatter(df_priv1[\"long\"], df_priv1[\"lat\"], color=\"green\", alpha=0.01, label=quartile)\n",
    "axes[1].set_title(\"With Privacy\")\n",
    "axes[1].set_xlabel(\"Longitude\")\n",
    "axes[1].set_ylabel(\"Latitude\")\n",
    "\n",
    "fig.suptitle(\"Location of Houses, With And Without Privacy\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "draw_rectangle(axes[0], x=-122.05, y=47.37, width=0.10, height=0.05, color=\"blue\")\n",
    "draw_rectangle(axes[0], x=-122.16, y=47.47, width=0.10, height=0.05, color=\"blue\")\n",
    "draw_rectangle(axes[0], x=-122.00, y=47.58, width=0.10, height=0.05, color=\"blue\")\n",
    "\n",
    "plt_savefig(\"privacy-plot\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
